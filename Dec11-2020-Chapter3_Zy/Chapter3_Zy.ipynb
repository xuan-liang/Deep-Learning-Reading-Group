{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xuan-liang/Deep-Learning-Reading-Group/blob/main/Dec11-2020-Chapter3_Zy/Chapter3_Zy.ipynb)\n",
    "\n",
    "## Some useful links for Keras\n",
    "* <a href=\"https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean\">Sample, Batch, Epoch</a>\n",
    "* <a href=\"https://keras.io/api/layers/core_layers/dense/\">Dense layers</a>\n",
    "* <a href=\"https://keras.io/api/losses/\">Loss functions</a>\n",
    "* <a href=\"https://keras.io/api/optimizers/\">Optimizers</a>\n",
    "* <a href=\"https://keras.io/api/metrics/\">Metrics</a>\n",
    "* `tf.keras.backend.clear_session()` <a href=\"https://keras.io/api/utils/backend_utils/\">documentation</a> and <a href=\"https://stackoverflow.com/questions/50895110/what-do-i-need-k-clear-session-and-del-model-for-keras-with-tensorflow-gpu\">discussion</a>.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud GPU\n",
    "* <a href=\"https://colab.research.google.com\">Google Colab</a>\n",
    "* <a href=\"https://console.aws.amazon.com/ec2/v2\">Amazon Elastic Compute Cloud (Amazon EC2)</a>\n",
    "* <a href=\"https://matpool.com/\">Matpool (矩池云)</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Today's Talk\n",
    "* Anatomy of a neural network\n",
    "* Keras\n",
    "* Overview of typical Keras workflow\n",
    "* Binary classification example\n",
    "* Single-label multi-class classification example\n",
    "* Regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a neural network\n",
    "\n",
    "* Layers, which are combined into a network (or model)\n",
    "* The input data $X$ and corresponding targets $Y$\n",
    "* The loss function, which defines the feedback signal used for learning\n",
    "* The optimizer, which determines how learning proceeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: the building blocks of deep learning\n",
    "\n",
    "A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Different layers are appropriate for different tensor formats and different types of data\n",
    "processing.\n",
    "\n",
    "* simple vector data, stored in 2D tensors of shape (samples,features), is often processed by densely connected layers, also called fully connected or dense layers (the *Dense* class in Keras). We will mainly deal with this in this talk.\n",
    "* sequence data, stored in 3D tensors of shape (samples,timesteps, features), is typically processed by recurrent layers such as an *LSTM* layer.\n",
    "* Image data, stored in 4D tensors, is usually processed by 2D convolution layers (*Conv2D*).\n",
    "\n",
    "Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. When using Keras, you don’t have to worry about\n",
    "compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `network.summary()` to see the output shapes of every layer, including the final output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "print(network.input_shape)\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge output1 = relu(dot(W1, input) + b1)$$\n",
    "$$\\Huge output2 = softmax(dot(W2, output1) + b2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the first layer (dense_1) will accept as input 2D tensors with shape (n,28*28), and return a tensor with shape (n,512). Similarly, the second layer (dense_2) automatically infer its input shape (n,512) from the first layer, and return a tensor with shape (n,10) as the final output. https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get object attributes in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_inbound_node',\n",
       " '_add_unique_metric_name',\n",
       " '_base_init',\n",
       " '_build_input_shape',\n",
       " '_built',\n",
       " '_cache_output_metric_attributes',\n",
       " '_check_trainable_weights_consistency',\n",
       " '_compute_previous_mask',\n",
       " '_expects_training_arg',\n",
       " '_feed_input_names',\n",
       " '_feed_input_shapes',\n",
       " '_feed_inputs',\n",
       " '_get_callback_model',\n",
       " '_get_existing_metric',\n",
       " '_get_node_attribute_at_index',\n",
       " '_get_training_eval_metrics',\n",
       " '_handle_metrics',\n",
       " '_handle_per_output_metrics',\n",
       " '_inbound_nodes',\n",
       " '_init_graph_network',\n",
       " '_init_subclassed_network',\n",
       " '_initial_weights',\n",
       " '_input_coordinates',\n",
       " '_input_layers',\n",
       " '_is_compiled',\n",
       " '_is_graph_network',\n",
       " '_layers',\n",
       " '_layers_by_depth',\n",
       " '_losses',\n",
       " '_make_predict_function',\n",
       " '_make_test_function',\n",
       " '_make_train_function',\n",
       " '_metrics',\n",
       " '_network_nodes',\n",
       " '_node_key',\n",
       " '_nodes_by_depth',\n",
       " '_non_trainable_weights',\n",
       " '_outbound_nodes',\n",
       " '_output_coordinates',\n",
       " '_output_layers',\n",
       " '_output_mask_cache',\n",
       " '_output_shape_cache',\n",
       " '_output_tensor_cache',\n",
       " '_per_input_losses',\n",
       " '_per_input_updates',\n",
       " '_prepare_total_loss',\n",
       " '_set_inputs',\n",
       " '_set_metric_attributes',\n",
       " '_set_per_output_metric_attributes',\n",
       " '_set_sample_weight_attributes',\n",
       " '_standardize_user_data',\n",
       " '_trainable_weights',\n",
       " '_updated_config',\n",
       " '_updates',\n",
       " '_uses_dynamic_learning_phase',\n",
       " '_uses_inputs_arg',\n",
       " '_validate_or_infer_batch_size',\n",
       " 'add',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'add_weight',\n",
       " 'assert_input_compatibility',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compile',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'count_params',\n",
       " 'dtype',\n",
       " 'evaluate',\n",
       " 'evaluate_generator',\n",
       " 'fit',\n",
       " 'fit_generator',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_layer',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_names',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'inputs',\n",
       " 'layers',\n",
       " 'load_weights',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'metrics_names',\n",
       " 'model',\n",
       " 'name',\n",
       " 'non_trainable_weights',\n",
       " 'optimizer',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_names',\n",
       " 'output_shape',\n",
       " 'outputs',\n",
       " 'pop',\n",
       " 'predict',\n",
       " 'predict_classes',\n",
       " 'predict_generator',\n",
       " 'predict_on_batch',\n",
       " 'predict_proba',\n",
       " 'reset_metrics',\n",
       " 'reset_states',\n",
       " 'run_internal_graph',\n",
       " 'save',\n",
       " 'save_weights',\n",
       " 'set_weights',\n",
       " 'state_updates',\n",
       " 'stateful',\n",
       " 'summary',\n",
       " 'supports_masking',\n",
       " 'test_on_batch',\n",
       " 'to_json',\n",
       " 'to_yaml',\n",
       " 'train_on_batch',\n",
       " 'trainable',\n",
       " 'trainable_weights',\n",
       " 'updates',\n",
       " 'uses_learning_phase',\n",
       " 'weights']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " '_is_compiled': False,\n",
       " '_expects_training_arg': False,\n",
       " '_initial_weights': None,\n",
       " 'supports_masking': False,\n",
       " 'optimizer': None,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_losses': [],\n",
       " '_per_input_losses': {},\n",
       " '_per_input_updates': {},\n",
       " '_metrics': [],\n",
       " '_layers': [<keras.engine.input_layer.InputLayer at 0x24f06b63828>,\n",
       "  <keras.layers.core.Dense at 0x24f06b634a8>,\n",
       "  <keras.layers.core.Dense at 0x24f7e010da0>],\n",
       " '_outbound_nodes': [],\n",
       " '_inbound_nodes': [<keras.engine.base_layer.Node at 0x24f06b63b70>],\n",
       " '_is_graph_network': True,\n",
       " '_uses_inputs_arg': True,\n",
       " 'outputs': [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>],\n",
       " 'inputs': [<tf.Tensor 'dense_1_input:0' shape=(?, 784) dtype=float32>],\n",
       " '_built': True,\n",
       " '_build_input_shape': None,\n",
       " '_compute_previous_mask': True,\n",
       " '_input_layers': [<keras.engine.input_layer.InputLayer at 0x24f06b63828>],\n",
       " '_output_layers': [<keras.layers.core.Dense at 0x24f7e010da0>],\n",
       " '_input_coordinates': [(<keras.engine.input_layer.InputLayer at 0x24f06b63828>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_coordinates': [(<keras.layers.core.Dense at 0x24f7e010da0>, 0, 0)],\n",
       " '_output_mask_cache': {'2538438410704_1895996720': None},\n",
       " '_output_tensor_cache': {},\n",
       " '_output_shape_cache': {},\n",
       " '_network_nodes': {'dense_1_ib-0', 'dense_1_input_ib-0', 'dense_2_ib-0'},\n",
       " '_nodes_by_depth': {0: [<keras.engine.base_layer.Node at 0x24f06bd1128>],\n",
       "  1: [<keras.engine.base_layer.Node at 0x24f06b63e80>],\n",
       "  2: [<keras.engine.base_layer.Node at 0x24f06b63860>]},\n",
       " '_layers_by_depth': {0: [<keras.layers.core.Dense at 0x24f7e010da0>],\n",
       "  1: [<keras.layers.core.Dense at 0x24f06b634a8>],\n",
       "  2: [<keras.engine.input_layer.InputLayer at 0x24f06b63828>]},\n",
       " 'input_names': ['dense_1_input'],\n",
       " 'output_names': ['dense_2'],\n",
       " '_feed_input_names': ['dense_1_input'],\n",
       " '_feed_inputs': [<tf.Tensor 'dense_1_input:0' shape=(?, 784) dtype=float32>],\n",
       " '_feed_input_shapes': [(None, 784)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above uses the *Sequential* class (only for linear stacks of layers, which is the most common network architecture by far) in to define the two-layer model. Another way to define a model is by using the functional API (for directed acyclic graphs of layers, which lets you build completely arbitrary architectures). More details on functional API are given in Chapter 7. Once your model architecture is defined, it doesn’t matter whether you used a Sequential model or the functional API. The remaining steps (configuring loss function/optimizer/metric and learning) are the same.\n",
    "\n",
    "With the functional API, you’re manipulating the data tensors that the model processes and applying layers to this tensor as if they were functions. Below is the same model defined using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(512, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "network_API = models.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "network_API.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and optimizers: keys to configuring the learning process\n",
    "\n",
    "Loss function (objective function)—The quantity that will be minimized during training. It represents a measure of success for the task at hand. \n",
    "\n",
    "Some examples of loss function for common problems are:\n",
    "* Binary crossentropy for a two-class classification problem, ie. $-y \\log(\\hat{p}) - (1-y)\\log(1-\\hat{p}) $\n",
    "* Categorical crossentropy for a $K$-class classification problem where $K > 2$, ie. $-\\sum_{k=1}^{K} y_k \\log(\\hat{p}_k)$\n",
    "* Mean-squared error for a regression problem\n",
    "* Connectionist temporal classification (CTC) for a sequence learning problem. \n",
    "\n",
    "More loss functions can be found <a href=\"https://keras.io/api/losses/\">here</a>.\n",
    "\n",
    "Optimizer—Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).  \n",
    "\n",
    "Some examples of optimizers are:\n",
    "* SGD with momentum\n",
    "* Adagrad\n",
    "* RMSProp \n",
    "\n",
    "The RMSProp optimizer is generally a good enough choice. More optimizers can be found <a href=\"https://keras.io/api/optimizers/\">here</a>.\n",
    "\n",
    "A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that any loss function may be used as a metric.\n",
    "\n",
    "Some examples of metrics are:\n",
    "* Accuracy\n",
    "* True/False positives & negatives\n",
    "* Mean squared error\n",
    "\n",
    "More metris can be found <a href=\"https://keras.io/api/metrics/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are passing our optimizer, loss function and metrics as strings, which is possible because `rmsprop`, `categorical_crossentropy` and `accuracy` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss function or metric function. The former can be done by passing an optimizer class instance as the `optimizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "network.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter can be done by passing function objects as the `loss` or `metrics` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "network.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.categorical_crossentropy,\n",
    "              metrics=[metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "<figure>\n",
    "    <img src='image/keras.jpg'/>\n",
    "    <figcaption>The deep-learning\n",
    "software and hardware stack</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras relies on a specialized, well-optimized tensor library to handle low-level operations (eg. tensor manipulation and differentiation), serving as the **backend engine** of Keras. Several different backend engines can be plugged seamlessly into Keras. Currently, the three existing backend implementations are the **TensorFlow** backend, the **Theano** backend, and the Microsoft Cognitive Toolkit (**CNTK**) backend.\n",
    "<br><br>\n",
    "Any piece of code that you write with Keras can be run with any of these backends without having to change anything in the code: you can seamlessly switch between the two during development, which often proves useful—for instance, if one of these backends\n",
    "proves to be faster for a specific task. \n",
    "<br><br>\n",
    "Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both CPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-level library for tensor operations called Eigen (http://eigen.tuxfamily.org). On GPU, TensorFlow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (cuDNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that TensorFlow is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otherwise, run `conda install tensorflow` in the terminal. If you have a GPU with CUDA support, `conda install tensorflow-gpu` to install TensorFlow with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Keras TensorFlow backend is running on GPU or CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3405722018220665943\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 16338367508376815324)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the following function to check whether TensorFlow can access a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available() # True/False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or only check for GPU's with CUDA support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(cuda_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Options to Run Keras\n",
    "* If you have a high-end NVIDIA GPU, install everything from scratch on a local Unix workstation. See Appendix A of the textbook. You can then run either local Jupyter notebooks or a regular Python codebase. \n",
    "* If not, use the official EC2 Deep Learning AMI (https://console.aws.amazon.com/ec2/v2), and run Keras experiments as Jupyter notebooks on EC2. See Appendix B of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Jupyter notebooks on an EC2 GPU instance (Paid)\n",
    "1. Create an account at https://portal.aws.amazon.com/billing/signup?nc2=h_ct&src=default&redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation#/start.\n",
    "2. Login as Root user at https://console.aws.amazon.com/ec2/v2.\n",
    "3. If the region on top right corner is not 'Sydney', change the region to Asia Pacific (Sydney).\n",
    "4. Click on 'Launch Instance', then follow the steps in Appendix B.3 of textbook to launch an instance. (Note that after selecting AWS Marketplace in Step 2 of Appendix B.3, you need to manually search for 'AWS Deep Learning AMI (Ubuntu 18.04)' and select the first product.)\n",
    "\n",
    "If the launching fail, then it could be due to your current vCPU limit is set to be 0 for running On-Demand P instances. In that case, you need to go to https://ap-southeast-2.console.aws.amazon.com/servicequotas/home?region=ap-southeast-2#!/ and click on Amazon Elastic Compute Cloud (Amazon EC2). Then，scroll down and look for ‘Running On-Demand P instances' then request quota increase (top right corner) for it. A single p2.xlarge instance will need 4 vCPU, so you will need to change the quota value $\\geq 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab at https://colab.research.google.com (Free)\n",
    "* Navigate to 'Edit' -> 'Notebook Settings', and select GPU as hardware accelerator. \n",
    "* Run Jupyter notebooks.\n",
    "* Colab resources are not guaranteed and not unlimited, and the usage limits sometimes fluctuate.\n",
    "* <a href=\"https://research.google.com/colaboratory/faq.html\">Google Colab FAQ</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matpool (矩池云）at https://matpool.com/ (Paid)\n",
    "* After registering an account, go to https://matpool.com/host-market and select your preferred GPU.\n",
    "* Limited availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing with Keras: a quick overview\n",
    "The typical Keras workflow is:\n",
    "\n",
    "1. Define training data: input tensors and target tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define a network of layers (or model) that maps inputs to targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Iterate on the training data by calling the fit() method of the model (usually also involves the use of validation set to tune the hyperparameters first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\tools\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2579 - accuracy: 0.9250\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1048 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0684 - accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0497 - accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0370 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f07206b70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie reviews: a binary classification example\n",
    "## The IMDB dataset\n",
    "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
    "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
    "will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
    "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List is one of the built-in data structure in Python. Other types of built-in data structure include tuple, set, dictionary, etc. \n",
    "<figure>\n",
    "    <img src='image/py_data_structure.jpeg'/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word index correspond to a word defined in a dictionary, where the index gives the rarity of the word (larger = more rare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the most common word is given as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(list(word_index.keys())[list(word_index.values()).index(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can quickly decode one of these reviews back to English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesn’t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitães',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'white’s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jōb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmé',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doña',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above reversed word index, we can convert the list of review back into English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the data\n",
    "\n",
    "\n",
    "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:\n",
    "\n",
    "* We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape `(samples, word_indices)`, \n",
    "then use as first layer in our network a layer capable of handling such integer tensors (the `Embedding` layer, which will be covered in  detail later in the book). https://keras.io/api/layers/core_layers/embedding/\n",
    "* We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence \n",
    "`[3, 5]` into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as \n",
    "first layer in our network a `Dense` layer, capable of handling floating point vector data.\n",
    "\n",
    "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our samples look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building our network\n",
    "Our input data is simply vectors, and our labels are scalars (1s and 0s): A type of network that performs well on such a problem would be a simple stack of fully-connected (`Dense`) layers with `relu` activations: `Dense(16, activation='relu')`\n",
    "\n",
    "The argument being passed to each `Dense` layer (16) is the number of \"hidden units\" of the layer.  It's a dimension in the representation space of the layer. You may remember from the previous chapter that each such `Dense` layer with a `relu` activation implements \n",
    "the following chain of tensor operations:\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can intuitively understand the dimensionality of your representation space as \"how much freedom you are allowing the network to have when learning internal representations\". Having more hidden units (a higher-dimensional representation space) allows your network to learn more complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data), ie. **overfitting**.\n",
    "\n",
    "There are two key architecture decisions to be made about such stack of dense layers:\n",
    "\n",
    "* How many layers to use.\n",
    "* How many \"hidden units\" to chose for each layer.\n",
    "\n",
    "In this case, we will have the following architecture choice:\n",
    "two intermediate layers with 16 hidden units each, \n",
    "and a third layer which will output the scalar prediction regarding the sentiment of the current review. \n",
    "The intermediate layers will use `relu` as their \"activation function\", \n",
    "and the final layer will use a sigmoid activation so as to output a probability \n",
    "(a score between 0 and 1, indicating how likely the sample is to have the target \"1\", i.e. how likely the review is to be positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='image/3_layer_network.png'/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10000)\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring the learning process\n",
    "Since we are dealing with binary classification problem, we can use `binary_crossentropy` loss function. As for the optimizer, we configure our model with the `rmsprop` optimizer. Finally, we monitor `accuracy` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit the model together with validation\n",
    "In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a \"validation set\" by setting apart 10,000 samples from the original training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train our model for 20 epochs (20 iterations over all samples in the `partial_x_train` and `partial_y_train` tensors), in mini-batches of 512 samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the validation data as the `validation_data` argument. The purpose of doing this is to help select/tune some hyperparameters, eg. no. of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.5117 - accuracy: 0.8039 - val_loss: 0.4348 - val_accuracy: 0.8069\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.3122 - accuracy: 0.9007 - val_loss: 0.3108 - val_accuracy: 0.8856\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.2266 - accuracy: 0.9273 - val_loss: 0.2858 - val_accuracy: 0.8884\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.1801 - accuracy: 0.9427 - val_loss: 0.2803 - val_accuracy: 0.8886\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.1471 - accuracy: 0.9529 - val_loss: 0.2829 - val_accuracy: 0.8889\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.3079 - val_accuracy: 0.8800\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.1000 - accuracy: 0.9714 - val_loss: 0.3317 - val_accuracy: 0.8795\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.0811 - accuracy: 0.9773 - val_loss: 0.3542 - val_accuracy: 0.8720\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0676 - accuracy: 0.9825 - val_loss: 0.3521 - val_accuracy: 0.8824\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.0552 - accuracy: 0.9855 - val_loss: 0.3882 - val_accuracy: 0.8771\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.0488 - accuracy: 0.9881 - val_loss: 0.4041 - val_accuracy: 0.8742\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 73us/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.4302 - val_accuracy: 0.8730\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.0309 - accuracy: 0.9936 - val_loss: 0.4611 - val_accuracy: 0.8713\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 73us/step - loss: 0.0238 - accuracy: 0.9959 - val_loss: 0.4937 - val_accuracy: 0.8688\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.0215 - accuracy: 0.9959 - val_loss: 0.5251 - val_accuracy: 0.8714\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 73us/step - loss: 0.0183 - accuracy: 0.9966 - val_loss: 0.5536 - val_accuracy: 0.8691\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.0098 - accuracy: 0.9994 - val_loss: 0.5868 - val_accuracy: 0.8669\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 65us/step - loss: 0.0108 - accuracy: 0.9989 - val_loss: 0.6297 - val_accuracy: 0.8630\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.6533 - val_accuracy: 0.8644\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.0044 - accuracy: 0.9999 - val_loss: 0.7071 - val_accuracy: 0.8654\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On CPU, this will take less than two seconds per epoch -- training is over in 20 seconds. At the end of every epoch, there is a slight pause \n",
    "as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
    "\n",
    "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data about everything that happened during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 4 entries: one per metric that was being monitored, during training and during validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss decreases with every epoch and the training accuracy increases with every epoch, which is expected when running gradient descent optimization -- the quantity we are trying to minimize should get lower with every iteration. But that \n",
    "isn't the case for the validation loss and accuracy: they seem to peak at the third/fourth epoch. This is an example of **overfitting**: after the second epoch, we are over-optimizing on the training data, and we \n",
    "ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
    "\n",
    "In this case, to prevent overfitting, we could simply stop training after three/four epochs. Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.4704 - accuracy: 0.8152\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 42us/step - loss: 0.2733 - accuracy: 0.9085\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 42us/step - loss: 0.2070 - accuracy: 0.9268\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 42us/step - loss: 0.1730 - accuracy: 0.9394\n",
      "25000/25000 [==============================] - 2s 71us/step\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model2.add(layers.Dense(16, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2914161449432373, 0.8840799927711487]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our approach gives an accuracy of 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions for new data\n",
    "After training the network, we can use the network to generate the probability of reviews being positive by using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1387018 ],\n",
       "       [0.99812734],\n",
       "       [0.88641465],\n",
       "       ...,\n",
       "       [0.11579096],\n",
       "       [0.08029947],\n",
       "       [0.6045158 ]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is very confident for some samples (0.99) but less confident for others (0.65). Other settings (no. of layers/hidden units, loss function, activation function) can be tried too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* In a binary classification problem (two output classes), your network should end with a Dense layer with 1 unit and a sigmoid activation, i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "* With such a scalar sigmoid output, on a binary classification problem, the loss function that can be used is `binary_crossentropy`.\n",
    "* As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data never-seen-before. Make sure to always monitor performance on validation data that is outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires: a single-label multi-class classification example\n",
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, widely used toy dataset for text classification. There are $N$ = 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each newswire is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
       "       list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
       "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
       "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
       "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using similar code, we can convert the list of integers back into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with a newswire is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the data\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the labels, there are two possibilities: \n",
    "* we could just cast the label list as an integer tensor\n",
    "* we could use a \"one-hot\" encoding or equivalently \"categorical encoding\". \n",
    "\n",
    "In our case, one-hot encoding of our labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which have been used in previous MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(one_hot_train_labels.shape)\n",
    "one_hot_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building our network\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\n",
    "\n",
    "We will use \n",
    "* Two intermediate `Dense` layers with 64 hidden units and `relu` activation function\n",
    "* An output (third) layer with 46 units and `softmax` activation function\n",
    "\n",
    "Hence, for each input sample, our network will output a 46-dimensional vector, which contains the *probability distribution* over the 46 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model3 = models.Sequential()\n",
    "model3.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring the learning process\n",
    "Since we are dealing with multiclass classification problem, we can use `categorical_crossentropy` loss function. As for the optimizer, we configure our model with the `rmsprop` optimizer. Finally, we monitor `accuracy` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit the model together with validation\n",
    "Set apart 1,000 samples in training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 2.8036 - accuracy: 0.5050 - val_loss: 1.8592 - val_accuracy: 0.6180\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 0s 62us/step - loss: 1.4977 - accuracy: 0.6954 - val_loss: 1.3229 - val_accuracy: 0.7190\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 1.0886 - accuracy: 0.7657 - val_loss: 1.1395 - val_accuracy: 0.7590\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 76us/step - loss: 0.8598 - accuracy: 0.8185 - val_loss: 1.0293 - val_accuracy: 0.7870\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 0s 60us/step - loss: 0.6935 - accuracy: 0.8571 - val_loss: 0.9698 - val_accuracy: 0.7980\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 0s 60us/step - loss: 0.5597 - accuracy: 0.8849 - val_loss: 0.9209 - val_accuracy: 0.8200\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.4546 - accuracy: 0.9050 - val_loss: 0.9026 - val_accuracy: 0.8110\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 68us/step - loss: 0.3730 - accuracy: 0.9208 - val_loss: 0.9033 - val_accuracy: 0.8130\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.3071 - accuracy: 0.9337 - val_loss: 0.8978 - val_accuracy: 0.8180\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 69us/step - loss: 0.2581 - accuracy: 0.9425 - val_loss: 0.9074 - val_accuracy: 0.8260\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 0.2266 - accuracy: 0.9479 - val_loss: 0.9211 - val_accuracy: 0.8210\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 73us/step - loss: 0.1952 - accuracy: 0.9488 - val_loss: 0.9479 - val_accuracy: 0.8260\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 0s 59us/step - loss: 0.1740 - accuracy: 0.9531 - val_loss: 0.9837 - val_accuracy: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 68us/step - loss: 0.1594 - accuracy: 0.9546 - val_loss: 1.0100 - val_accuracy: 0.8130\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 0.1453 - accuracy: 0.9551 - val_loss: 1.0209 - val_accuracy: 0.8040\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 76us/step - loss: 0.1378 - accuracy: 0.9554 - val_loss: 1.0131 - val_accuracy: 0.8160\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 0s 59us/step - loss: 0.1252 - accuracy: 0.9573 - val_loss: 1.1219 - val_accuracy: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 0s 62us/step - loss: 0.1231 - accuracy: 0.9575 - val_loss: 1.0472 - val_accuracy: 0.8080\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 0s 58us/step - loss: 0.1201 - accuracy: 0.9560 - val_loss: 1.0729 - val_accuracy: 0.8120\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 0s 57us/step - loss: 0.1107 - accuracy: 0.9582 - val_loss: 1.0904 - val_accuracy: 0.8110\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 (or 9) epochs. So, we train a new network from scratch for 8 epochs, then evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "8982/8982 [==============================] - 1s 88us/step - loss: 2.3939 - accuracy: 0.5514\n",
      "Epoch 2/8\n",
      "8982/8982 [==============================] - 1s 62us/step - loss: 1.3100 - accuracy: 0.7199\n",
      "Epoch 3/8\n",
      "8982/8982 [==============================] - 0s 55us/step - loss: 0.9885 - accuracy: 0.7909\n",
      "Epoch 4/8\n",
      "8982/8982 [==============================] - 1s 59us/step - loss: 0.7765 - accuracy: 0.8351\n",
      "Epoch 5/8\n",
      "8982/8982 [==============================] - 0s 55us/step - loss: 0.6122 - accuracy: 0.8727\n",
      "Epoch 6/8\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.4876 - accuracy: 0.8988\n",
      "Epoch 7/8\n",
      "8982/8982 [==============================] - 0s 55us/step - loss: 0.3941 - accuracy: 0.9177\n",
      "Epoch 8/8\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.3235 - accuracy: 0.9282\n",
      "2246/2246 [==============================] - 0s 160us/step\n"
     ]
    }
   ],
   "source": [
    "model4 = models.Sequential()\n",
    "model4.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model4.fit(x_train,one_hot_train_labels,epochs=8,batch_size=512)\n",
    "results = model4.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9582945192274194, 0.7938557267189026]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach reaches an accuracy of 79%. For comparison, we find the accuracy of purely random classifier below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17853962600178094"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02315227070347284"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_labels_rand = np.zeros(len(test_labels))\n",
    "for i in range(len(test_labels)):\n",
    "    test_labels_rand[i] = random.randint(0,45)\n",
    "\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_rand))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the results of our approach seem pretty good, at least when compared to a random baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions for new data\n",
    "We can use the trained network to generate the probability of different classes by using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14622548e-04, 6.17780897e-04, 4.34426292e-05, ...,\n",
       "        5.58998763e-05, 1.81160758e-05, 2.51830770e-05],\n",
       "       [4.45784582e-03, 3.01638409e-03, 2.37399456e-03, ...,\n",
       "        1.00476056e-04, 2.47545213e-05, 3.67548782e-04],\n",
       "       [6.30520517e-03, 7.85011351e-01, 2.05503264e-03, ...,\n",
       "        2.07020843e-04, 1.89444894e-04, 6.57863216e-04],\n",
       "       ...,\n",
       "       [9.90419139e-05, 2.97751656e-04, 5.44243994e-06, ...,\n",
       "        5.79880107e-05, 1.34298398e-05, 2.06907189e-05],\n",
       "       [1.83790596e-03, 4.62396219e-02, 1.26261672e-03, ...,\n",
       "        1.95248739e-03, 1.14858500e-03, 1.14723796e-03],\n",
       "       [2.59570486e-04, 6.10827565e-01, 1.32756317e-02, ...,\n",
       "        1.57585790e-04, 1.94635912e-04, 2.63955386e-04]], dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model4.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities in each 46-dimensional vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss\n",
    "As mentioned above, another way to encode the labels would be to cast them as an integer tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden units. We can try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than 46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 111us/step - loss: 3.5937 - accuracy: 0.1312 - val_loss: 3.3212 - val_accuracy: 0.3400\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 3.0806 - accuracy: 0.4010 - val_loss: 2.8511 - val_accuracy: 0.4470\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 2.5809 - accuracy: 0.4731 - val_loss: 2.4023 - val_accuracy: 0.4750\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 2.1246 - accuracy: 0.5133 - val_loss: 2.0328 - val_accuracy: 0.5360\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 1.7661 - accuracy: 0.6220 - val_loss: 1.7654 - val_accuracy: 0.6200\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 1.5033 - accuracy: 0.6949 - val_loss: 1.5825 - val_accuracy: 0.6720\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 0s 62us/step - loss: 1.3162 - accuracy: 0.7230 - val_loss: 1.4705 - val_accuracy: 0.6770\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 1.1838 - accuracy: 0.7373 - val_loss: 1.4012 - val_accuracy: 0.6840\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 71us/step - loss: 1.0858 - accuracy: 0.7497 - val_loss: 1.3567 - val_accuracy: 0.6850\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 1.0131 - accuracy: 0.7623 - val_loss: 1.3240 - val_accuracy: 0.6990\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 0s 62us/step - loss: 0.9490 - accuracy: 0.7736 - val_loss: 1.3112 - val_accuracy: 0.7050\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 70us/step - loss: 0.8991 - accuracy: 0.7790 - val_loss: 1.3078 - val_accuracy: 0.7060\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 68us/step - loss: 0.8527 - accuracy: 0.7861 - val_loss: 1.2997 - val_accuracy: 0.7040\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 71us/step - loss: 0.8138 - accuracy: 0.7885 - val_loss: 1.3053 - val_accuracy: 0.6990\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.7765 - accuracy: 0.7937 - val_loss: 1.3143 - val_accuracy: 0.6990\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 0.7447 - accuracy: 0.7969 - val_loss: 1.3336 - val_accuracy: 0.6960\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.7144 - accuracy: 0.8027 - val_loss: 1.3343 - val_accuracy: 0.7000\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.6848 - accuracy: 0.8140 - val_loss: 1.3438 - val_accuracy: 0.7020\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 0.6594 - accuracy: 0.8167 - val_loss: 1.3685 - val_accuracy: 0.7000\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 0.6332 - accuracy: 0.8237 - val_loss: 1.3794 - val_accuracy: 0.7020\n"
     ]
    }
   ],
   "source": [
    "model5 = models.Sequential()\n",
    "model5.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model5.add(layers.Dense(4, activation='relu'))\n",
    "model5.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model5.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history =model5.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network now seems to peak at ~71% validation accuracy, an 8-9% absolute drop compared to the previous network with 64 units intermediate layers. This drop is mostly due to the fact that we are now trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram *most* of the necessary information into these 4-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgU1bnH8e/LgMCwuABuIDNiXCIiAxkJChpckqtIlBA3MhENRkRN3HKjRlRQQ3K9msSg0QR3vRMx0cQYRWNQEY1LBEQiiorKKAqKKDAIKOB7/zg10DTdMz3MdFdP9+/zPPV0dW39TtHU2+ecqnPM3RERkeLVKu4AREQkXkoEIiJFTolARKTIKRGIiBQ5JQIRkSKnRCAiUuSUCKRZmdkjZnZKc28bJzNbaGZHZOG4bmZfieZ/b2aXZbLtVnxOlZk9trVx1nPcIWa2qLmPK7nXOu4AJH5mtirhbSnwObAhen+Gu1dneix3Pyob2xY6dx/bHMcxs3LgHaCNu6+Pjl0NZPxvKMVHiUBw945182a2EPihu09L3s7MWtddXESkcKhqSNKqK/qb2UVmtgS43cy2N7OHzGypmX0azfdI2Ge6mf0wmj/VzJ4xs2ujbd8xs6O2ctvdzWyGmdWa2TQz+52Z/V+auDOJ8Soz+1d0vMfMrGvC+pPNrMbMlpnZuHrOz0AzW2JmJQnLvmNmc6P5AWb2nJktN7PFZnaDmW2T5lh3mNnPE97/NNrnAzMbnbTt0Wb2kpmtNLP3zGxCwuoZ0etyM1tlZgfWnduE/Q8ysxfNbEX0elCm56Y+ZvbVaP/lZjbPzI5JWDfUzF6Njvm+mf13tLxr9O+z3Mw+MbOnzUzXpRzTCZeG7AzsAJQBYwjfmduj9z2BNcAN9ez/deB1oCvwv8CtZmZbse0fgX8DXYAJwMn1fGYmMX4P+AGwI7ANUHdh2he4KTr+rtHn9SAFd38e+Aw4LOm4f4zmNwDnR3/PgcDhwFn1xE0Uw5FRPN8E9gSS2yc+A0YB2wFHA2ea2fBo3SHR63bu3tHdn0s69g7Aw8Ck6G/7NfCwmXVJ+hu2ODcNxNwG+DvwWLTfj4FqM9s72uRWQjVjJ2A/4Ilo+U+ARUA3YCfgEkD93uSYEoE05EtgvLt/7u5r3H2Zu9/v7qvdvRaYCHyjnv1r3P1md98A3AnsQvgPn/G2ZtYTOAC43N2/cPdngAfTfWCGMd7u7m+4+xrgT0BFtPw44CF3n+HunwOXRecgnXuAkQBm1gkYGi3D3We5+/Puvt7dFwJ/SBFHKidE8b3i7p8REl/i3zfd3f/j7l+6+9zo8zI5LoTE8aa73x3FdQ8wH/h2wjbpzk19BgIdgf+J/o2eAB4iOjfAOmBfM+vs7p+6++yE5bsAZe6+zt2fdnWAlnNKBNKQpe6+tu6NmZWa2R+iqpOVhKqI7RKrR5IsqZtx99XRbMdGbrsr8EnCMoD30gWcYYxLEuZXJ8S0a+KxowvxsnSfRfj1P8LM2gIjgNnuXhPFsVdU7bEkiuMXhNJBQzaLAahJ+vu+bmZPRlVfK4CxGR637tg1SctqgO4J79OdmwZjdvfEpJl43O8SkmSNmT1lZgdGy68BFgCPmdnbZnZxZn+GNCclAmlI8q+znwB7A193985sqopIV93THBYDO5hZacKy3erZvikxLk48dvSZXdJt7O6vEi54R7F5tRCEKqb5wJ5RHJdsTQyE6q1EfySUiHZz922B3ycct6Ff0x8QqswS9QTezyCuho67W1L9/sbjuvuL7n4sodroAUJJA3evdfefuHsvQqnkAjM7vImxSCMpEUhjdSLUuS+P6pvHZ/sDo1/YM4EJZrZN9Gvy2/Xs0pQY7wOGmdngqGH3Shr+f/JH4BxCwvlzUhwrgVVmtg9wZoYx/Ak41cz2jRJRcvydCCWktWY2gJCA6iwlVGX1SnPsqcBeZvY9M2ttZicC+xKqcZriBULbxYVm1sbMhhD+jaZE/2ZVZratu68jnJMNAGY2zMy+ErUF1S3fkPojJFuUCKSxrgPaAx8DzwOP5uhzqwgNrsuAnwP3Ep53SGWrY3T3ecDZhIv7YuBTQmNmfe4BhgBPuPvHCcv/m3CRrgVujmLOJIZHor/hCUK1yRNJm5wFXGlmtcDlRL+uo31XE9pE/hXdiTMw6djLgGGEUtMy4EJgWFLcjebuXwDHEEpGHwM3AqPcfX60ycnAwqiKbCzw/Wj5nsA0YBXwHHCju09vSizSeKZ2GWmJzOxeYL67Z71EIlLoVCKQFsHMDjCzPcysVXR75bGEumYRaSI9WSwtxc7AXwgNt4uAM939pXhDEikMqhoSESlyqhoSESlyLa5qqGvXrl5eXh53GCIiLcqsWbM+dvduqda1uERQXl7OzJkz4w5DRKRFMbPkJ8o3UtWQiEiRUyIQESlySgQiIkWuxbURiEjurVu3jkWLFrF27dqGN5ZYtWvXjh49etCmTZuM91EiEJEGLVq0iE6dOlFeXk76cYUkbu7OsmXLWLRoEbvvvnvG+xVF1VB1NZSXQ6tW4bVaw3iLNMratWvp0qWLkkCeMzO6dOnS6JJbwZcIqqthzBhYHQ1pUlMT3gNUVcUXl0hLoyTQMmzNv1PBlwjGjduUBOqsXh2Wi4hIESSCd99t3HIRyT/Lli2joqKCiooKdt55Z7p3777x/RdffFHvvjNnzuScc85p8DMOOuigZol1+vTpDBs2rFmOlSsFnwh6Jg/y18ByEWm65m6X69KlC3PmzGHOnDmMHTuW888/f+P7bbbZhvXr16fdt7KykkmTJjX4Gc8++2zTgmzBCj4RTJwIpaWbLystDctFpPnVtcvV1ID7pna55r5J49RTT+WCCy7g0EMP5aKLLuLf//43Bx10EP369eOggw7i9ddfBzb/hT5hwgRGjx7NkCFD6NWr12YJomPHjhu3HzJkCMcddxz77LMPVVVV1PXSPHXqVPbZZx8GDx7MOeec0+Av/08++YThw4ez//77M3DgQObOnQvAU089tbFE069fP2pra1m8eDGHHHIIFRUV7Lfffjz99NPNe8LqkbXGYjNrB8wA2kafc1/yaFLRuKZ/A96JFv3F3a9szjjqGoTHjQvVQT17hiSghmKR7KivXa65/9+98cYbTJs2jZKSElauXMmMGTNo3bo106ZN45JLLuH+++/fYp/58+fz5JNPUltby957782ZZ565xT33L730EvPmzWPXXXdl0KBB/Otf/6KyspIzzjiDGTNmsPvuuzNy5MgG4xs/fjz9+vXjgQce4IknnmDUqFHMmTOHa6+9lt/97ncMGjSIVatW0a5dOyZPnsx//dd/MW7cODZs2MDq5JOYRdm8a+hz4DB3X2VmbYBnzOwRd38+abun3T2rFWpVVbrwi+RKLtvljj/+eEpKSgBYsWIFp5xyCm+++SZmxrp161Luc/TRR9O2bVvatm3LjjvuyIcffkiPHj0222bAgAEbl1VUVLBw4UI6duxIr169Nt6fP3LkSCZPnlxvfM8888zGZHTYYYexbNkyVqxYwaBBg7jggguoqqpixIgR9OjRgwMOOIDRo0ezbt06hg8fTkVFRZPOTWNkrWrIg1XR2zbRpFFwRApcLtvlOnTosHH+sssu49BDD+WVV17h73//e9p76du2bbtxvqSkJGX7QqpttmYQr1T7mBkXX3wxt9xyC2vWrGHgwIHMnz+fQw45hBkzZtC9e3dOPvlk7rrrrkZ/3tbKahuBmZWY2RzgI+Cf7v5Cis0ONLOXzewRM+udzXhEJPviapdbsWIF3bt3B+COO+5o9uPvs88+vP322yxcuBCAe++9t8F9DjnkEKqjxpHp06fTtWtXOnfuzFtvvUWfPn246KKLqKysZP78+dTU1LDjjjty+umnc9pppzF79uxm/xvSyWoicPcN7l4B9AAGmNl+SZvMBsrcvS9wPWkGIzezMWY208xmLl26NJshi0gTVVXB5MlQVgZm4XXy5OxXz1544YX87Gc/Y9CgQWzYsKHZj9++fXtuvPFGjjzySAYPHsxOO+3EtttuW+8+EyZMYObMmey///5cfPHF3HnnnQBcd9117LfffvTt25f27dtz1FFHMX369I2Nx/fffz/nnntus/8N6eRszGIzGw985u7X1rPNQqDS3T9Ot01lZaVrYBqR3Hrttdf46le/GncYsVu1ahUdO3bE3Tn77LPZc889Of/88+MOawup/r3MbJa7V6baPmslAjPrZmbbRfPtgSOA+Unb7GzR89BmNiCKZ1m2YhIRaYqbb76ZiooKevfuzYoVKzjjjDPiDqlZZPOuoV2AO82shHCB/5O7P2RmYwHc/ffAccCZZrYeWAOc5LkqooiINNL555+flyWApspaInD3uUC/FMt/nzB/A3BDtmIQEZGGFfyTxSIiUj8lAhGRIqdEICJS5JQIRCTvDRkyhH/84x+bLbvuuus466yz6t2n7lbzoUOHsnz58i22mTBhAtdem/aOdgAeeOABXn311Y3vL7/8cqZNm9aY8FPKp+6qlQhEJO+NHDmSKVOmbLZsypQpGXX8BqHX0O22226rPjs5EVx55ZUcccQRW3WsfFU0iWDNGrj77tAtroi0LMcddxwPPfQQn3/+OQALFy7kgw8+YPDgwZx55plUVlbSu3dvxo8fn3L/8vJyPv44PKc6ceJE9t57b4444oiNXVVDeEbggAMOoG/fvnz3u99l9erVPPvsszz44IP89Kc/paKigrfeeotTTz2V++67D4DHH3+cfv360adPH0aPHr0xvvLycsaPH0///v3p06cP8+fP3zKoBHF3V13wYxbXmTIFRo+GLl1g6NC4oxFpuc47D+bMad5jVlTAddelX9+lSxcGDBjAo48+yrHHHsuUKVM48cQTMTMmTpzIDjvswIYNGzj88MOZO3cu+++/f8rjzJo1iylTpvDSSy+xfv16+vfvz9e+9jUARowYwemnnw7ApZdeyq233sqPf/xjjjnmGIYNG8Zxxx232bHWrl3LqaeeyuOPP85ee+3FqFGjuOmmmzjvvPMA6Nq1K7Nnz+bGG2/k2muv5ZZbbkn798XdXXXRlAi+/33YfXe4/HKVCkRaosTqocRqoT/96U/079+ffv36MW/evM2qcZI9/fTTfOc736G0tJTOnTtzzDHHbFz3yiuvcPDBB9OnTx+qq6uZN29evfG8/vrr7L777uy1114AnHLKKcyYMWPj+hEjRgDwta99bWNHdek888wznHzyyUDq7qonTZrE8uXLad26NQcccAC33347EyZM4D//+Q+dOnWq99iZKJoSQZs2cNlloVTw4INw7LFxRyTSMtX3yz2bhg8fzgUXXMDs2bNZs2YN/fv355133uHaa6/lxRdfZPvtt+fUU09N2/10nahXmy2ceuqpPPDAA/Tt25c77riD6dOn13uchjpBqOvKOl1X1w0dq6676qOPPpqpU6cycOBApk2btrG76ocffpiTTz6Zn/70p4waNare4zekaEoEACefDHvuGUoFX34ZdzQi0hgdO3ZkyJAhjB49emNpYOXKlXTo0IFtt92WDz/8kEceeaTeYxxyyCH89a9/Zc2aNdTW1vL3v/9947ra2lp22WUX1q1bt7HraIBOnTpRW1u7xbH22WcfFi5cyIIFCwC4++67+cY3vrFVf1vc3VUXTYkAoHVrGD8+VBPdfz8cf3zcEYlIY4wcOZIRI0ZsrCLq27cv/fr1o3fv3vTq1YtBgwbVu3///v058cQTqaiooKysjIMPPnjjuquuuoqvf/3rlJWV0adPn40X/5NOOonTTz+dSZMmbWwkBmjXrh233347xx9/POvXr+eAAw5g7NixW/V3TZgwgR/84Afsv//+lJaWbtZd9ZNPPklJSQn77rsvRx11FFOmTOGaa66hTZs2dOzYsVkGsMlZN9TNpandUG/YAH36hH7S586FaJQ7EamHuqFuWfKmG+p8VVICEybAq69CBgMMiYgUvKJLBADHHRdKBRMmQANtOCIiBa8oE0GrVnDFFfDmm5DQJiQi9Whp1cjFamv+nYoyEQAMHw79+8OVV8K6dXFHI5Lf2rVrx7Jly5QM8py7s2zZMtq1a9eo/YrqrqFEZiEJDBsGd94JP/xh3BGJ5K8ePXqwaNEili5dGnco0oB27drRo0ePRu1TdHcNJXKHAw+ExYvhjTcgev5DRKTg6K6hNOpKBe++C7feGnc0IiLxKOpEAPDNb8LgwTBxIjTwZLqISEEq+kRQVyr44AP4wx9Sb1NdDeXl4W6j8nLdaSQihaXoEwHAoYeG6Ze/hOQeXaurYcwYqKkJbQo1NeG9koGIFAolgsiVV8KHH8KNN26+fNy4LZPD6tVhuYhIIVAiiAweDN/6Flx9NSR2NPjuu6m3T7dcRKSlUSJIcNVV8PHHcP31m5b17Jl623TLRURamqwlAjNrZ2b/NrOXzWyemV2RYhszs0lmtsDM5ppZ/2zFk4kBA8IDZtdeCytWhGUTJ0Jp6ebblZaG5SIihSCbJYLPgcPcvS9QARxpZgOTtjkK2DOaxgA3ZTGejFxxBXz66aZRmKqqYPJkKCsLdxiVlYX3VVXxxiki0lyylgg8WBW9bRNNyY8xHwvcFW37PLCdme2SrZgy0b8/fOc78Otfh4QA4aK/cGEY1WzhQiUBESksWW0jMLMSM5sDfAT8091fSNqkO/BewvtF0bLk44wxs5lmNjMXfZ1ccQWsXAm/+lXWP0pEJHZZTQTuvsHdK4AewAAz2y9pk1SjSG/R+ZG7T3b3Snev7NatWzZC3UyfPnDCCfDb34bGYxGRQpaTu4bcfTkwHTgyadUiYLeE9z2AD3IRU0MmTIDPPoNrrok7EhGR7MrmXUPdzGy7aL49cAQwP2mzB4FR0d1DA4EV7r44WzE1xle/Ct/7HtxwQ3jQTESkUGWzRLAL8KSZzQVeJLQRPGRmY81sbLTNVOBtYAFwM3BWFuNptPHj4fPP4X/+J+5IRESyp6jHI8jE6NHwxz/C22/Drrvm7GNFRJqVxiNogssugw0b4Be/iDsSEZHsUCJowO67h1LBzTerfyERKUxKBBmo62lU3UqISCFSIshAz55w+ulw223w1ltxRyMi0ryUCDI0bhy0aQOXXx53JCIizUuJIEO77ALnnRfuIJozJ+5oRESajxJBI1x4IWy/PfzsZ3FHIiLSfJQIGmG77UISePRRmD497mhERJqHEkEj/ehH0KMHXHxxGMxeRKSlUyJopPbtQ4d0L7wADzwQdzQiIk2nRLAVTjkF9tkHLrkE1q+POxoRkaZRItgKrVuHLifmz4c774w7GhGRplEi2ErDh8PAgaGH0jVr4o5GRGTrKRFsJbPQPfX774cxC0REWiolgib4xjfgyCPhl7+E5cvjjkZEZOsoETTRL38Jn34KV18ddyQiIltHiaCJKirCkJa//S18kBejLYuINI4SQTO46qpwG+mVV8YdiYhI4ykRNINeveCMM+CWW+CNN+KORkSkcZQImsmll0K7duE1WXU1lJdDq1bhtbo619GJiKSnRNBMdtoJfvIT+POfYebMTcurq2HMGKipCX0T1dSE90oGIpIvzFtYz2mVlZU+M/FKm0dWroQ99oC+fWHatLCsvDxc/JOVlcHChbmMTkSKmZnNcvfKVOtUImhGnTuHkcwefxz++c+wLN2A9+mWi4jkmhJBMzvzzPBr/+KL4csvw3jHqaRbLiKSa1lLBGa2m5k9aWavmdk8Mzs3xTZDzGyFmc2JphY/InDbtuE20tmz4b77YOJEKC3dfJvS0rBcRCQftM7isdcDP3H32WbWCZhlZv9091eTtnva3YdlMY6cq6qCa64J1USvRn/tuHGhOqhnz5AEqqrijVFEpE7WEoG7LwYWR/O1ZvYa0B1ITgQFp6QkdD3x7W/DrbfC2LG68ItI/spJG4GZlQP9gBdSrD7QzF42s0fMrHea/ceY2Uwzm7l06dIsRtp8jj4aBg2CK66Azz6LOxoRkfSyngjMrCNwP3Ceu69MWj0bKHP3vsD1QMrBH919srtXuntlt27dshtwMzELHdEtWRL6IRIRyVdZTQRm1oaQBKrd/S/J6919pbuviuanAm3MrGs2Y8qlQYNC9dDVV8OyZXFHIyKSWjbvGjLgVuA1d/91mm12jrbDzAZE8RTUJfMXv4Da2tBmICKSj7JZIhgEnAwclnB76FAzG2tmY6NtjgNeMbOXgUnASd7SHnVuwH77wahRYRSz996LOxoRkS2pi4kcqKmBvfaC738/3EUkIpJr6mIiZmVlcPbZcMcd8PLLcUcjIrI5JYIcGTcOunSB004Lg9iIiOQLJYIc6dIltBPMmgW/+U3c0YiIbKJEkEPHHw/Dh8Pll8Obb8YdjYhIoESQQ2bwu9+FjulOPz30TioiEjclghzbdVf41a/gqadg8uS4oxERUSKIxejRcPjhcOGFerZAROKnRBADs1Aa2LAhDGTTwh7lEJECo0QQk169wrgEDz8M99wTdzQiUsyUCGL04x/D178O55wDLaR3bREpQEoEMSopCV1OrFwZkoGISByUCGLWuzdceilMmQIPPhh3NCJSjDJKBGbWwcxaRfN7mdkx0VgD0gwuvhj69AkNxytWxB2NiBSbTEsEM4B2ZtYdeBz4AXBHtoIqNttsE6qIliwJt5SKiORSponA3H01MAK43t2/A+ybvbCKzwEHwAUXhNtKn3wy7mhEpJhknAjM7ECgCng4WtY6OyEVryuugD32CN1PrF4ddzQiUiwyTQTnAT8D/uru88ysF6Dfrc2stBRuuQXeeit0TFenuhrKy6FVq/BaXR1XhCJSiBo9QlnUaNzR3VdmJ6T6tcQRyhpr7Fi4+WZ47rnQS+mYMZuXEEpLQxVSVVV8MYpIy1LfCGUZJQIz+yMwFtgAzAK2BX7t7tc0Z6CZKIZEsGJFuK10++3DMwbvvrvlNmVlsHBhzkMTkRaqOYaq3DcqAQwHpgI9CQPTSxZsuy3cdBO88krqJADpl4uINFamiaBN9NzAcOBv7r4OUFdpWfTtb8PIkenX9+yZu1hEpLBlmgj+ACwEOgAzzKwMiKWNoJj89rfQqVNoJE5UWho6rBMRaQ4ZJQJ3n+Tu3d19qAc1wKFZjq3odesGf/hDGMls++1D99VlZWooFpHmlWkXE9ua2a/NbGY0/YpQOpAsO+kkGDYM1q6FBQtCA7GSgIg0p0yrhm4DaoETomklcHt9O5jZbmb2pJm9ZmbzzOzcFNuYmU0yswVmNtfM+jf2Dyh0ZqHhuHXr8KCZBrERkeaWaSLYw93Hu/vb0XQF0KuBfdYDP3H3rwIDgbPNLLlbiqOAPaNpDHBTI2IvGj16wDXXwBNPbP6gmYhIc8g0Eawxs8F1b8xsELCmvh3cfbG7z47ma4HXgO5Jmx0L3BW1OzwPbGdmu2QcfREZMwZ++EP4+c9DCUFEpLlk2l/QWOAuM9s2ev8pcEqmH2Jm5UA/4IWkVd2BxOHbF0XLFmd67GJRV0W0ZAmcfTbstBOMGBF3VCJSCDK9a+hld+8L7A/s7+79gMMy2dfMOgL3A+el6JbCUn1cimOMqWuoXlrEYzq2bg333huGt/ze9+Dpp+OOSEQKQaNGKHP3lQkX8wsa2j56CO1+oNrd/5Jik0XAbgnvewAfpPjcye5e6e6V3bp1a0zIBae0FB56KHQ+d8wxMG9e3BGJSEvXlKEqU/2a37TSzIBbgdfc/ddpNnsQGBXdPTQQWOHuqhZqQJcu8Oij0L49HHkkvPdew/uIiKTTlETQ0I2Mgwj9ER1mZnOiaaiZjTWzsdE2U4G3gQXAzcBZTYinqJSXwyOPhE7pjjwSPv007ohEpKWqt7HYzGpJfcE3oH19+7r7MzRQavDQ9enZDcQoafTtCw88EBLBscfCP/4RSgkiIo1Rb4nA3Tu5e+cUUyd31whleeDQQ+Huu+GZZ8ITxxs2xB2RiLQ0Takakjxxwglw3XXw17/Cj3+sp49FpHH0q75AnHMOvP8+/O//QvfuMG5c3BGJSEuhRFBAfvlLWLwYLr0UdtkFRo+OOyIRaQmUCApIq1Zw663w0UehS4qddoKjj447KhHJd2ojKDBt2sB990FFBRx/PLyQ3KmHiEgSJYIC1LEjPPww7LprKBG8/nrcEYlIPlMiKFA77RSeKygpgYMPDl1Zt2oVHkSrro47OhHJJ0oEBWyPPcLtpEuXhjuK3KGmJrQfKBmISB0lggJ3yy1bLlu9WreXisgmSgQF7t13G7dcRIqPEkGB69kz9fIuXXIbh4jkLyWCAjdxYhjDIFGrVvDxx3DuufDFF/HEJSL5Q4mgwFVVweTJUFYWhrssK4PbbgtJYNIkOOQQVROJFDvzFtZDWWVlpc+cOTPuMArCffeFbijatIH/+z846qi4IxKRbDGzWe5emWqdSgRF7LjjYNas8IzB0KHhTqL16+OOSkRyTYmgyO25Jzz/PJx2GvziF/DNb8KSJXFHJSK5pEQgtG8fnje4447QN1G/fvDUU3FHJSK5okQgG51ySkgEnTvDYYeFbq2//DLuqEQk25QIZDN9+sDMmaH94JJL4Jhj4JNP4o5KRLJJiUC20KkTTJkC118Pjz0G/fvDiy/GHZWIZIsSgaRkBj/6ETzzTOisbtAguOEGVRWJFCIlAqnXgAHw0kvwrW+Fnkz33x/uugvWrYs7MhFpLkoE0qAddoAHHwwPnZmFRuU99oDf/hY++yzu6ESkqZQIJCOtWoXuKubODaOflZfDeeeFTu0mTAh9F4lIy5S1RGBmt5nZR2b2Spr1Q8xshZnNiabLsxWLNF11dbj4l5TAWWfBGWfAv/4FgwfDFVeEPozOPTcMfCMiLUs2SwR3AEc2sM3T7l4RTVdmMRZpgurqMKpZTc3mo5y98w787W/wyitw/PFw443wla/AqFFhmYi0DFlLBO4+A9Ad6AVg3LgwqlmixFHOevcOTyW/9Va40+j++8PzCN/+drjrSETyW9xtBAea2ctm9oiZ9U63kZmNMbOZZjZz6dKluYxPyHyUs5494Te/CcuvuAKeew4OPjhUHz30kG49FclXcSaC2UCZu/cFrgceSLehu09290p3r+zWrVvOApQg3ctQpysAAA5XSURBVChn9Y1+dvnloQpp0iR4771QOujTB666KvR4qqQgkj9iSwTuvtLdV0XzU4E2ZtY1rngkvVSjnJWWhuX16dAhPHuwYEF49qBTJxg/HiorYddd4Qc/gD//GZYvz17sItKw2BKBme1sZhbND4hiWRZXPJJeqlHOJk8OyzPRpg2cfHLo7nrJkpAUDj00NDSfcAJ07Qrf+AZcfTX85z+hQVpEcidrI5SZ2T3AEKAr8CEwHmgD4O6/N7MfAWcC64E1wAXu/mxDx9UIZYVj/Xr4979h6tQwvfRSWF43UM7QoXD44dCxY7xxihSC+kYo01CVkjc++AAefTQkhcceg9pa2GabMK7y0KFhKM299w6lEhFpHCUCaXG++AKefXZTaWHevLC8c+fQ31FFBfTtG6b99guD64hIekoE0uLV1MC0aaH6aM6c0NVFbW1Y16pVKCn07bspQVRUwM47xxuzSCrr14c+ulatavg1edmwYTBy5NZ9bn2JoHVT/iCRXCkrC+Mq1/nyS1i4MCSFl18Or889F8ZRqLPjjptKDXUJ4itfgXbtch6+FLgvvwz9bX3wQZjefz/164oV8PnnmR/XLLSRdegQXvv3z078KhFITlRXhyeR3303PH8wcWLmdx01xqefhtJCXXJ4+eVQrZT4n69799B7aqpphx2aPyZpOdzDd2X16s2nzz4L00cfpb7IL168ZdfsZuHHyK67bpq2337zC3vifPJrx47hR0tztYmpakhiVddXUWI3FaWljbsFtSnWrYPXXw8JYsGC0BVG3bRkyebbbrdd+iTRvXuohpLs+fzz8Mv6449h6dJNr0uXwrJl4d/SfdMEjXu/du2WF/nkKZNLYufO4cLevXv61513DrdO5wslAolVeXnqXknLykL1Tpw++wzefnvz5FA31dSE+tw6bdqEX3SdOoULQefOjZvv1Ckco3XrMJWUbD7f0pPMl1/CmjXhYpr8Wjd98smmC3uqi31du08ys3Du27bd9L5uasz7du3Cj5DEqUOHLZelm+p+4bfEW5rVRiCxyrSvojh06BC6vujTZ8t169eH7jHqEsM774SnoFeuDBeslSth0aJN87W14Rfn1jLbMjkkz7drF6b27cPUmPl27WDDhnBH1tZO9V3oG1P33bYtdOsWpq5dQ4mra9fNlyW+7rBDOAeSHUoEknU9e6YuEaTrqyhftG4Nu+8epiOOyGyfL74ICaEuOSQmjdraULWxfn24ICe+pptPXLZ+fUg0a9eGC/CaNaG6pG4+cfnatY1/QrukJDy3Ud/Uvj1su22o9igtDe/rfi3Xzde3bIcdwoW9Qwc9D5JPlAgk6yZOTN1G0FBfRS3RNtuETve6dIk3DvdNv+DrEsTatSG5pbrAt2nT8qumZOspEUjW1TUI5+KuIQnMQvVLXZ26SH2UCCQnqqp04RfJVyoMiogUOSUCaRGqq8NtqK1ahdfq6rgjEikcqhqSvJf8QFpNTXgPqm4SaQ4qEUjeGzdu8zuOILwfNy6eeEQKjRKB5L18fiBNpBAoEUjeS/fgWb4/kCbSUigRSN6bODE8gJaoUB9IE4mDEoHkvaqq0FNpWVl4UKqsLHc9l4oUAyUCaRGqqkJPpXUD0jQ2Cej2U5H0dPuoFDzdfipSP5UIpODp9lOR+ikRSMHT7aci9VMikIKn209F6pe1RGBmt5nZR2b2Spr1ZmaTzGyBmc01s/7ZikWKW3PcfqrGZilk2SwR3AEcWc/6o4A9o2kMcFMWY5Ei1tTbT+sam2tqwoAvdY3NSgZSKLI6eL2ZlQMPuft+Kdb9AZju7vdE718Hhrj74vqOqcHrJdfKy1MPtVlWFm5lFWkJ6hu8Ps42gu7AewnvF0XLtmBmY8xsppnNXLp0aU6CE6mjxmYpdHEmglRDV6csnrj7ZHevdPfKbt26ZTkskc01R2Oz2hgkn8WZCBYBuyW87wF8EFMsImk1tbFZbQyS7+JMBA8Co6K7hwYCKxpqHxCJQ1Mbm/VAm+S7bN4+eg/wHLC3mS0ys9PMbKyZjY02mQq8DSwAbgbOylYsIk3VlL6OmqONQVVLkk1Z62vI3Uc2sN6Bs7P1+SL5omfP1HcdZdrGoL6SJNv0ZLFIljW1jUFVS5JtSgQiWdbUNgZVLUm2qRtqkRyoqtr6ahxVLUm2qUQgkudUtSTZpkQgkudUtSTZpqohkRZAVUuSTSoRiBS4fKhaUokivykRiBS4uKuW1MVG/lMiECkCTXkyuqmd7qlEkf+UCESkXk2tWlKJIv8pEYhIvZpataQSRf5TIhCRBjWlakklivynRCAiWaUSRf5TIhCRrFOJIr8pEYhIXlOJIvuUCEQk76lEkV1KBCJS0FSiaJgSgYgUPJUo6qdEICJSj0IoUTREiUBEpAEtuUSRCSUCEZEsirtEkQklAhGRLIuzRJEJJQIRkTzW1BJFJjRCmYhInmvKCHWZUIlARKTIZTURmNmRZva6mS0ws4tTrB9iZivMbE40XZ7NeEREZEtZqxoysxLgd8A3gUXAi2b2oLu/mrTp0+4+LFtxiIhI/bJZIhgALHD3t939C2AKcGwWP09ERLZCNhNBd+C9hPeLomXJDjSzl83sETPrnepAZjbGzGaa2cylS5dmI1YRkaKVzbuGLMUyT3o/Gyhz91VmNhR4ANhzi53cJwOTAcxsqZnVNHewzaQr8HHcQdQj3+OD/I9R8TWN4muapsRXlm5FNhPBImC3hPc9gA8SN3D3lQnzU83sRjPr6u5p/1B379bskTYTM5vp7pVxx5FOvscH+R+j4msaxdc02Yovm1VDLwJ7mtnuZrYNcBLwYOIGZrazmVk0PyCKZ1kWYxIRkSRZKxG4+3oz+xHwD6AEuM3d55nZ2Gj974HjgDPNbD2wBjjJ3ZOrj0REJIuy+mSxu08FpiYt+33C/A3ADdmMIccmxx1AA/I9Psj/GBVf0yi+pslKfKYf4CIixU1dTIiIFDklAhGRIqdE0EhmtpuZPWlmr5nZPDM7N8U2sfahZGYLzew/0WfPTLHezGxS1AfUXDPrn8PY9k44L3PMbKWZnZe0Tc7Pn5ndZmYfmdkrCct2MLN/mtmb0ev2afatt0+tLMZ3jZnNj/4N/2pm26XZt97vQxbjm2Bm7yf8Ow5Ns29c5+/ehNgWmtmcNPtm9fylu6bk9Pvn7poaMQG7AP2j+U7AG8C+SdsMAR6KMcaFQNd61g8FHiE89DcQeCGmOEuAJYSHCmM9f8AhQH/glYRl/wtcHM1fDFyd5m94C+gFbAO8nPx9yGJ83wJaR/NXp4ovk+9DFuObAPx3Bt+BWM5f0vpfAZfHcf7SXVNy+f1TiaCR3H2xu8+O5muB10jddUY+Oxa4y4Pnge3MbJcY4jgceMvdY39S3N1nAJ8kLT4WuDOavxMYnmLXnPSplSo+d3/M3ddHb58nPLQZizTnLxOxnb860bNMJwD3NPfnZqKea0rOvn9KBE1gZuVAP+CFFKsb7EMpixx4zMxmmdmYFOsz7Qcq204i/X++OM9fnZ3cfTGE/6zAjim2yZdzOZpQykuloe9DNv0oqrq6LU3VRj6cv4OBD939zTTrc3b+kq4pOfv+KRFsJTPrCNwPnOcJXWVE6vpQ6gtcT+hDKZcGuXt/4CjgbDM7JGl9Jv1AZVX0tPkxwJ9TrI77/DVGPpzLccB6oDrNJg19H7LlJmAPoAJYTKh+SRb7+QNGUn9pICfnr4FrStrdUixr9PlTItgKZtaG8A9W7e5/SV7v7ivdfVU0PxVoY2ZdcxWfu38QvX4E/JVQfEzUYD9QOXAUMNvdP0xeEff5S/BhXZVZ9PpRim1iPZdmdgowDKjyqNI4WQbfh6xw9w/dfYO7fwncnOZz4z5/rYERwL3ptsnF+UtzTcnZ90+JoJGi+sRbgdfc/ddptomtDyUz62BmnermCQ2KryRt9iAwKrp7aCCwoq4ImkNpf4XFef6SPAicEs2fAvwtxTYN9qmVLWZ2JHARcIy7r06zTSbfh2zFl9ju9J00nxvb+YscAcx390WpVubi/NVzTcnd9y9bLeGFOgGDCUWvucCcaBoKjAXGRtv8CJhHaMF/Hjgoh/H1ij735SiGcdHyxPiMMHrcW8B/gMocn8NSwoV924RlsZ4/QlJaDKwj/Mo6DegCPA68Gb3uEG27KzA1Yd+hhDs93qo73zmKbwGhfrjue/j75PjSfR9yFN/d0fdrLuHitEs+nb9o+R1137uEbXN6/uq5puTs+6cuJkREipyqhkREipwSgYhIkVMiEBEpckoEIiJFTolARKTIKRGIRMxsg23eM2qz9YRpZuWJPV+K5JOsDlUp0sKscfeKuIMQyTWVCEQaEPVHf7WZ/TuavhItLzOzx6NO1R43s57R8p0sjA/wcjQdFB2qxMxujvqcf8zM2kfbn2Nmr0bHmRLTnylFTIlAZJP2SVVDJyasW+nuA4AbgOuiZTcQuvPen9Dh26Ro+STgKQ+d5vUnPJEKsCfwO3fvDSwHvhstvxjoFx1nbLb+OJF09GSxSMTMVrl7xxTLFwKHufvbUedgS9y9i5l9TOg2YV20fLG7dzWzpUAPd/884RjlwD/dfc/o/UVAG3f/uZk9Cqwi9LL6gEcd7onkikoEIpnxNPPptknl84T5DWxqozua0PfT14BZUY+YIjmjRCCSmRMTXp+L5p8l9PYIUAU8E80/DpwJYGYlZtY53UHNrBWwm7s/CVwIbAdsUSoRySb98hDZpL1tPoD5o+5edwtpWzN7gfDjaWS07BzgNjP7KbAU+EG0/FxgspmdRvjlfyah58tUSoD/M7NtCb3C/sbdlzfbXySSAbURiDQgaiOodPeP445FJBtUNSQiUuRUIhARKXIqEYiIFDklAhGRIqdEICJS5JQIRESKnBKBiEiR+3+V8zzcGuG/5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after around 10 epochs. So, we train a new network from scratch for 10 epochs, then evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 1s 83us/step - loss: 3.1354 - accuracy: 0.2111\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 1s 59us/step - loss: 2.4093 - accuracy: 0.2325\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 1s 57us/step - loss: 2.0453 - accuracy: 0.5048\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 1s 60us/step - loss: 1.8020 - accuracy: 0.5793\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 0s 55us/step - loss: 1.6204 - accuracy: 0.6137\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 1s 59us/step - loss: 1.4773 - accuracy: 0.6283\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 0s 55us/step - loss: 1.3602 - accuracy: 0.6396\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 1s 62us/step - loss: 1.2594 - accuracy: 0.6540\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 1.1706 - accuracy: 0.6943\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 1s 58us/step - loss: 1.0896 - accuracy: 0.7211\n",
      "2246/2246 [==============================] - 0s 155us/step\n"
     ]
    }
   ],
   "source": [
    "model6 = models.Sequential()\n",
    "model6.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model6.add(layers.Dense(4, activation='relu'))\n",
    "model6.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model6.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model6.fit(x_train,one_hot_train_labels, epochs=10,batch_size=512)\n",
    "results = model6.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4574700234091293, 0.6696348786354065]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is ~67%, which is smaller than the test accuracy of 78% for previous network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* In a multi-class classification problem with N classes, your network should end with a `Dense` layer of size N.\n",
    "* For single-label multi-class classification problem, your network should end with a `softmax` activation, so that it will output a probability distribution over the N output classes.\n",
    "* `Categorical crossentropy` is almost always the loss function you should use for such problems.\n",
    "* There are two ways to handle labels in multi-class classification:<br>\n",
    "    * Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss function.<br>\n",
    "    * Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.<br>\n",
    "* If you need to classify data into a large number $N$ of categories, then you should avoid creating information bottlenecks in your network by having intermediate layers that are too small, ie. with much smaller number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices: a regression example\n",
    "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points (or features) about the suburb at the time, such as the crime rate, the local property tax rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as follow:\n",
    "\n",
    "1. Per capita crime rate.\n",
    "2. Proportion of residential land zoned for lots over 25,000 square feet.\n",
    "3. Proportion of non-retail business acres per town.\n",
    "4. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5. Nitric oxides concentration (parts per 10 million).\n",
    "6. Average number of rooms per dwelling.\n",
    "7. Proportion of owner-occupied units built prior to 1940.\n",
    "8. Weighted distances to five Boston employment centres.\n",
    "9. Index of accessibility to radial highways.\n",
    "10. Full-value property-tax rate per \\\\$10,000.\n",
    "11. Pupil-teacher ratio by town.\n",
    "12. $1000 \\times (Bk - 0.63)^2$ where Bk is the proportion of Black people by town.\n",
    "13. % lower status of the population.\n",
    "\n",
    "Two points to note\n",
    "* Small sample sizes\n",
    "* each feature has a different scale\n",
    "\n",
    "The targets are the median values of owner-occupied homes, in thousands of dollars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the data\n",
    "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our workflow any quantity computed on the test data, even for something as simple as data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building our network\n",
    "Because so few samples are available, we will be using a very small network:\n",
    "* Two hidden layers with 64 units and `relu` activation function\n",
    "* Output (third) layer with one unit with no activation, ie. linear layer (since we are dealing with regression problem)\n",
    "\n",
    "In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring the learning process\n",
    "Since we are dealing with regression problem, we can use `mse` loss function. As for the optimizer, we configure our model with the `rmsprop` optimizer. Finally, we monitor `mae` (Mean Absolute Error) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit the model together with validation\n",
    "Due to the small sample size we have, the validation set approach will results in high variance for the validation scores. So, we will use K-fold cross validation instead, to tune the hyperparameters (such as the number of epochs used for training)\n",
    "\n",
    "### K-fold cross validation\n",
    "It consists of splitting the available data into K partitions (typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining partition. The validation score for the model used would then be the average of the K validation scores obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2105789184570312, 2.710846185684204, 2.918855905532837, 2.737666606903076]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.644486904144287"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average is a much more reliable metric than any single of the four scores. In this case, we are off by around \\\\$2,650 on average, which is still significant considering that the prices range from \\\\$10,000 to \\\\$50,000.\n",
    "\n",
    "Next, we will train the network for a bit longer: 500 epochs. To keep track of how well the model did at each epoch, we will modify our training loop to save the per-epoch validation score log. This allows us to select/tune the no. of epoch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`K.clear_session()` is to removes all the nodes left over in TensorFlow graph from previous models, freeing memory and preventing slowdown. For more information, refer to this <a href=\"https://keras.io/api/utils/backend_utils/\">documentation</a> and <a href=\"https://stackoverflow.com/questions/50895110/what-do-i-need-k-clear-session-and-del-model-for-keras-with-tensorflow-gpu\">discussion</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5791752338409424,\n",
       " 3.282008171081543,\n",
       " 3.1494345664978027,\n",
       " 2.770390510559082,\n",
       " 2.3319308757781982,\n",
       " 2.9422237873077393,\n",
       " 2.6865267753601074,\n",
       " 2.2640604972839355,\n",
       " 2.90531063079834,\n",
       " 2.587739944458008,\n",
       " 2.1700379848480225,\n",
       " 2.251910448074341,\n",
       " 2.0766608715057373,\n",
       " 2.8083767890930176,\n",
       " 2.250520706176758,\n",
       " 1.9147086143493652,\n",
       " 2.0446689128875732,\n",
       " 2.0724081993103027,\n",
       " 2.2334096431732178,\n",
       " 2.176100254058838,\n",
       " 1.8378697633743286,\n",
       " 2.002094030380249,\n",
       " 2.0080626010894775,\n",
       " 1.9154367446899414,\n",
       " 1.9427293539047241,\n",
       " 2.2005553245544434,\n",
       " 1.9766371250152588,\n",
       " 2.0456931591033936,\n",
       " 2.0965747833251953,\n",
       " 1.8056042194366455,\n",
       " 1.8826115131378174,\n",
       " 1.9579873085021973,\n",
       " 2.2439589500427246,\n",
       " 1.9844326972961426,\n",
       " 1.9263333082199097,\n",
       " 2.4618964195251465,\n",
       " 1.9254381656646729,\n",
       " 1.957099199295044,\n",
       " 2.1111059188842773,\n",
       " 2.1003031730651855,\n",
       " 1.8874846696853638,\n",
       " 2.455251455307007,\n",
       " 2.307032346725464,\n",
       " 1.8998397588729858,\n",
       " 1.7969874143600464,\n",
       " 1.9974033832550049,\n",
       " 1.9105390310287476,\n",
       " 1.74688720703125,\n",
       " 1.7280545234680176,\n",
       " 2.0150861740112305,\n",
       " 1.738922119140625,\n",
       " 1.9017575979232788,\n",
       " 2.058232307434082,\n",
       " 1.8764095306396484,\n",
       " 2.186047315597534,\n",
       " 2.18790602684021,\n",
       " 2.1904728412628174,\n",
       " 2.2583541870117188,\n",
       " 1.8842273950576782,\n",
       " 1.9500070810317993,\n",
       " 1.8768020868301392,\n",
       " 2.2500710487365723,\n",
       " 1.8386365175247192,\n",
       " 1.9624165296554565,\n",
       " 1.7684365510940552,\n",
       " 1.8613064289093018,\n",
       " 1.7496150732040405,\n",
       " 1.9845629930496216,\n",
       " 2.092967987060547,\n",
       " 1.9055345058441162,\n",
       " 1.8668466806411743,\n",
       " 1.9720990657806396,\n",
       " 1.8532500267028809,\n",
       " 2.024137020111084,\n",
       " 1.9747542142868042,\n",
       " 1.894372820854187,\n",
       " 2.0747969150543213,\n",
       " 1.7462010383605957,\n",
       " 1.8294402360916138,\n",
       " 1.8235794305801392,\n",
       " 2.108152151107788,\n",
       " 1.9447067975997925,\n",
       " 2.0090951919555664,\n",
       " 1.9556703567504883,\n",
       " 1.934001088142395,\n",
       " 1.9458131790161133,\n",
       " 2.406447172164917,\n",
       " 2.0019264221191406,\n",
       " 1.8775018453598022,\n",
       " 1.8803037405014038,\n",
       " 1.8805134296417236,\n",
       " 2.052903652191162,\n",
       " 2.1369948387145996,\n",
       " 3.149740695953369,\n",
       " 1.858391523361206,\n",
       " 2.443019151687622,\n",
       " 1.9321941137313843,\n",
       " 1.8670179843902588,\n",
       " 1.999883770942688,\n",
       " 1.987219214439392,\n",
       " 2.1116459369659424,\n",
       " 2.3645129203796387,\n",
       " 2.0325136184692383,\n",
       " 2.1502230167388916,\n",
       " 1.9099667072296143,\n",
       " 2.047286033630371,\n",
       " 2.450395107269287,\n",
       " 1.8821580410003662,\n",
       " 1.9508233070373535,\n",
       " 1.9388126134872437,\n",
       " 2.058379650115967,\n",
       " 1.8802862167358398,\n",
       " 2.0535078048706055,\n",
       " 1.9935837984085083,\n",
       " 2.1575233936309814,\n",
       " 2.063701629638672,\n",
       " 2.3711724281311035,\n",
       " 2.139869213104248,\n",
       " 2.2316370010375977,\n",
       " 2.190664768218994,\n",
       " 2.0270259380340576,\n",
       " 1.93280029296875,\n",
       " 2.179893970489502,\n",
       " 2.1948208808898926,\n",
       " 2.034684658050537,\n",
       " 2.0605618953704834,\n",
       " 2.1785082817077637,\n",
       " 2.1207685470581055,\n",
       " 2.421750545501709,\n",
       " 2.099005937576294,\n",
       " 2.3638529777526855,\n",
       " 2.08416485786438,\n",
       " 2.333552837371826,\n",
       " 2.386157751083374,\n",
       " 2.7784981727600098,\n",
       " 2.2562263011932373,\n",
       " 2.1484932899475098,\n",
       " 2.1754510402679443,\n",
       " 2.3420677185058594,\n",
       " 2.0781350135803223,\n",
       " 2.280219554901123,\n",
       " 2.1452698707580566,\n",
       " 2.167097568511963,\n",
       " 2.1833622455596924,\n",
       " 2.2268624305725098,\n",
       " 2.0102434158325195,\n",
       " 2.4184176921844482,\n",
       " 2.251743793487549,\n",
       " 2.2493503093719482,\n",
       " 1.9292222261428833,\n",
       " 2.47579026222229,\n",
       " 2.1790196895599365,\n",
       " 2.088358163833618,\n",
       " 2.132024049758911,\n",
       " 2.105708360671997,\n",
       " 2.4694204330444336,\n",
       " 2.0938615798950195,\n",
       " 2.496171474456787,\n",
       " 2.1309280395507812,\n",
       " 2.3272061347961426,\n",
       " 2.186542272567749,\n",
       " 2.171877384185791,\n",
       " 2.1634275913238525,\n",
       " 2.225111246109009,\n",
       " 2.2663896083831787,\n",
       " 2.0938844680786133,\n",
       " 2.0330684185028076,\n",
       " 2.0369887351989746,\n",
       " 2.2178452014923096,\n",
       " 2.459367275238037,\n",
       " 2.155291795730591,\n",
       " 2.1470224857330322,\n",
       " 2.191699981689453,\n",
       " 2.4120373725891113,\n",
       " 2.1853888034820557,\n",
       " 2.186929702758789,\n",
       " 2.1252801418304443,\n",
       " 2.337480068206787,\n",
       " 2.212110996246338,\n",
       " 2.173569679260254,\n",
       " 2.3585450649261475,\n",
       " 2.5961804389953613,\n",
       " 2.164388418197632,\n",
       " 2.342726707458496,\n",
       " 2.271104097366333,\n",
       " 2.02889084815979,\n",
       " 2.0920181274414062,\n",
       " 2.377509593963623,\n",
       " 2.27871036529541,\n",
       " 2.216428518295288,\n",
       " 2.441277027130127,\n",
       " 2.196521520614624,\n",
       " 2.3816983699798584,\n",
       " 2.2060935497283936,\n",
       " 2.2682037353515625,\n",
       " 2.265897035598755,\n",
       " 2.8325676918029785,\n",
       " 2.1022539138793945,\n",
       " 2.6377005577087402,\n",
       " 2.7028114795684814,\n",
       " 2.287107229232788,\n",
       " 2.266188859939575,\n",
       " 2.4246954917907715,\n",
       " 2.462221622467041,\n",
       " 2.248748540878296,\n",
       " 2.101304531097412,\n",
       " 2.4918293952941895,\n",
       " 2.3524975776672363,\n",
       " 2.3019015789031982,\n",
       " 2.3307883739471436,\n",
       " 2.25715708732605,\n",
       " 2.1541099548339844,\n",
       " 2.2413277626037598,\n",
       " 2.3418455123901367,\n",
       " 2.46073842048645,\n",
       " 2.156857490539551,\n",
       " 2.070389986038208,\n",
       " 2.521451950073242,\n",
       " 2.284705638885498,\n",
       " 2.310053825378418,\n",
       " 2.193495273590088,\n",
       " 2.3483669757843018,\n",
       " 2.4550461769104004,\n",
       " 2.3727595806121826,\n",
       " 2.5432369709014893,\n",
       " 2.517828941345215,\n",
       " 2.165907859802246,\n",
       " 2.3958284854888916,\n",
       " 2.2953531742095947,\n",
       " 2.290480852127075,\n",
       " 2.4152767658233643,\n",
       " 2.3091399669647217,\n",
       " 2.4354217052459717,\n",
       " 2.560894012451172,\n",
       " 2.316500186920166,\n",
       " 2.272339105606079,\n",
       " 2.4260425567626953,\n",
       " 2.35621976852417,\n",
       " 2.198086738586426,\n",
       " 2.4278159141540527,\n",
       " 2.233855962753296,\n",
       " 2.1671817302703857,\n",
       " 2.3923845291137695,\n",
       " 2.391231060028076,\n",
       " 2.280634641647339,\n",
       " 2.16277813911438,\n",
       " 2.3488998413085938,\n",
       " 2.328523874282837,\n",
       " 2.2478368282318115,\n",
       " 2.4681925773620605,\n",
       " 2.450972080230713,\n",
       " 2.5794453620910645,\n",
       " 2.210883617401123,\n",
       " 2.410174608230591,\n",
       " 2.4200057983398438,\n",
       " 2.2475080490112305,\n",
       " 2.2668261528015137,\n",
       " 2.4324519634246826,\n",
       " 2.513864040374756,\n",
       " 2.472599744796753,\n",
       " 2.3883204460144043,\n",
       " 2.296868085861206,\n",
       " 2.1016695499420166,\n",
       " 2.412473201751709,\n",
       " 2.20329213142395,\n",
       " 2.3333678245544434,\n",
       " 2.1235930919647217,\n",
       " 2.3313398361206055,\n",
       " 2.3576674461364746,\n",
       " 2.1641249656677246,\n",
       " 2.283193826675415,\n",
       " 2.3830204010009766,\n",
       " 2.3334975242614746,\n",
       " 2.3642220497131348,\n",
       " 2.412884473800659,\n",
       " 2.6033835411071777,\n",
       " 2.4918103218078613,\n",
       " 2.220289707183838,\n",
       " 2.3130266666412354,\n",
       " 2.5631794929504395,\n",
       " 2.3747718334198,\n",
       " 2.2649085521698,\n",
       " 2.4829721450805664,\n",
       " 2.3765053749084473,\n",
       " 2.4778411388397217,\n",
       " 2.406062602996826,\n",
       " 2.7599456310272217,\n",
       " 2.32299542427063,\n",
       " 2.595271110534668,\n",
       " 2.3675692081451416,\n",
       " 2.309542417526245,\n",
       " 2.5357632637023926,\n",
       " 2.4111456871032715,\n",
       " 2.3493006229400635,\n",
       " 2.593036413192749,\n",
       " 2.5070724487304688,\n",
       " 2.3675143718719482,\n",
       " 2.2232182025909424,\n",
       " 2.1902410984039307,\n",
       " 2.413529396057129,\n",
       " 2.6267025470733643,\n",
       " 2.3974647521972656,\n",
       " 2.4936764240264893,\n",
       " 2.8010857105255127,\n",
       " 2.296611785888672,\n",
       " 2.5162503719329834,\n",
       " 2.430393695831299,\n",
       " 2.5446815490722656,\n",
       " 2.385378837585449,\n",
       " 2.4247050285339355,\n",
       " 2.3380069732666016,\n",
       " 2.264486074447632,\n",
       " 2.478358507156372,\n",
       " 2.375779867172241,\n",
       " 2.2805511951446533,\n",
       " 2.3010387420654297,\n",
       " 2.278932571411133,\n",
       " 2.5496373176574707,\n",
       " 2.8119616508483887,\n",
       " 2.327291250228882,\n",
       " 2.4445853233337402,\n",
       " 2.402310848236084,\n",
       " 2.3663930892944336,\n",
       " 2.2233681678771973,\n",
       " 2.5526013374328613,\n",
       " 2.1981024742126465,\n",
       " 2.4749534130096436,\n",
       " 2.349229574203491,\n",
       " 2.4645490646362305,\n",
       " 2.494426727294922,\n",
       " 2.435865640640259,\n",
       " 2.368459939956665,\n",
       " 2.445650577545166,\n",
       " 2.453662395477295,\n",
       " 2.461132287979126,\n",
       " 2.4238061904907227,\n",
       " 2.4799678325653076,\n",
       " 2.739922523498535,\n",
       " 2.5427334308624268,\n",
       " 2.4033427238464355,\n",
       " 2.4366021156311035,\n",
       " 2.3381175994873047,\n",
       " 2.462357997894287,\n",
       " 2.6802618503570557,\n",
       " 2.3752641677856445,\n",
       " 2.358886480331421,\n",
       " 2.474581480026245,\n",
       " 2.393571376800537,\n",
       " 2.389188051223755,\n",
       " 2.520073652267456,\n",
       " 2.412064790725708,\n",
       " 2.377004384994507,\n",
       " 2.366203784942627,\n",
       " 2.569636821746826,\n",
       " 2.3989346027374268,\n",
       " 2.3310654163360596,\n",
       " 2.3376991748809814,\n",
       " 2.4013848304748535,\n",
       " 2.099144697189331,\n",
       " 2.595409631729126,\n",
       " 2.1730942726135254,\n",
       " 2.3271493911743164,\n",
       " 2.4420406818389893,\n",
       " 2.3920154571533203,\n",
       " 2.4749112129211426,\n",
       " 2.41644549369812,\n",
       " 2.688951253890991,\n",
       " 2.5823252201080322,\n",
       " 2.3496785163879395,\n",
       " 2.3976705074310303,\n",
       " 2.4209210872650146,\n",
       " 2.6134860515594482,\n",
       " 2.563016891479492,\n",
       " 2.509510040283203,\n",
       " 2.412811040878296,\n",
       " 2.479823112487793,\n",
       " 2.3531413078308105,\n",
       " 2.7054600715637207,\n",
       " 2.3178908824920654,\n",
       " 2.247713327407837,\n",
       " 2.36063289642334,\n",
       " 2.417351484298706,\n",
       " 2.414569139480591,\n",
       " 2.381330966949463,\n",
       " 2.3904869556427,\n",
       " 2.6821866035461426,\n",
       " 2.246706485748291,\n",
       " 2.464169979095459,\n",
       " 2.501039505004883,\n",
       " 2.5066452026367188,\n",
       " 2.422377347946167,\n",
       " 2.4183804988861084,\n",
       " 2.3663477897644043,\n",
       " 2.5829617977142334,\n",
       " 2.3934519290924072,\n",
       " 2.583143472671509,\n",
       " 2.552250623703003,\n",
       " 2.3143792152404785,\n",
       " 2.3960461616516113,\n",
       " 2.3547749519348145,\n",
       " 2.4073402881622314,\n",
       " 2.6560094356536865,\n",
       " 2.4311227798461914,\n",
       " 2.441438674926758,\n",
       " 2.392740488052368,\n",
       " 2.32060170173645,\n",
       " 2.590506076812744,\n",
       " 2.3965444564819336,\n",
       " 2.505383253097534,\n",
       " 2.430682897567749,\n",
       " 2.4078309535980225,\n",
       " 2.191558599472046,\n",
       " 2.866480827331543,\n",
       " 2.3734943866729736,\n",
       " 2.1818642616271973,\n",
       " 2.2488698959350586,\n",
       " 2.5104572772979736,\n",
       " 2.55612850189209,\n",
       " 2.669750452041626,\n",
       " 2.298346757888794,\n",
       " 2.342625617980957,\n",
       " 2.524357318878174,\n",
       " 2.372924566268921,\n",
       " 2.452890157699585,\n",
       " 2.4637985229492188,\n",
       " 2.317932367324829,\n",
       " 2.5750982761383057,\n",
       " 2.279188394546509,\n",
       " 2.240863561630249,\n",
       " 2.2525017261505127,\n",
       " 2.28912091255188,\n",
       " 2.234360933303833,\n",
       " 2.384769916534424,\n",
       " 2.5391600131988525,\n",
       " 2.589094400405884,\n",
       " 2.3523495197296143,\n",
       " 2.484917163848877,\n",
       " 2.406440258026123,\n",
       " 2.38838267326355,\n",
       " 2.3767476081848145,\n",
       " 2.458306074142456,\n",
       " 2.4543375968933105,\n",
       " 2.320012092590332,\n",
       " 2.6613080501556396,\n",
       " 2.4999959468841553,\n",
       " 2.324835777282715,\n",
       " 2.627788782119751,\n",
       " 2.481677770614624,\n",
       " 2.631913185119629,\n",
       " 2.519813060760498,\n",
       " 2.3772194385528564,\n",
       " 2.5285282135009766,\n",
       " 2.424138069152832,\n",
       " 2.429769277572632,\n",
       " 2.1804072856903076,\n",
       " 2.2534759044647217,\n",
       " 2.5058014392852783,\n",
       " 2.3894259929656982,\n",
       " 2.400855541229248,\n",
       " 2.7272047996520996,\n",
       " 2.1393377780914307,\n",
       " 2.1842658519744873,\n",
       " 2.4336321353912354,\n",
       " 2.2637100219726562,\n",
       " 2.309100866317749,\n",
       " 2.494917869567871,\n",
       " 2.3160886764526367,\n",
       " 2.3791661262512207,\n",
       " 2.4286468029022217,\n",
       " 2.345951795578003,\n",
       " 2.6384072303771973,\n",
       " 2.6164438724517822,\n",
       " 2.197160482406616,\n",
       " 2.190049171447754,\n",
       " 2.390463352203369,\n",
       " 2.2934625148773193,\n",
       " 2.4181337356567383,\n",
       " 2.4431278705596924,\n",
       " 2.1580264568328857,\n",
       " 2.3405983448028564,\n",
       " 2.492236375808716,\n",
       " 2.328723669052124,\n",
       " 2.2417469024658203,\n",
       " 2.401089668273926,\n",
       " 2.2329189777374268,\n",
       " 2.5370428562164307,\n",
       " 2.5632705688476562,\n",
       " 2.191650629043579,\n",
       " 2.4678561687469482,\n",
       " 2.2933876514434814,\n",
       " 2.5198259353637695,\n",
       " 2.296499490737915,\n",
       " 2.2796812057495117,\n",
       " 2.344907760620117,\n",
       " 2.3129446506500244,\n",
       " 2.3311991691589355,\n",
       " 2.5905585289001465,\n",
       " 2.2984752655029297,\n",
       " 2.5259647369384766,\n",
       " 2.368234395980835]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mae_histories[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the average of the per-epoch MAE scores for all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let's:\n",
    "\n",
    "* Omit the first 10 data points, which are on a different scale from the rest of the curve.\n",
    "* Replace each point with an exponential moving average of the previous points, to obtain a smooth curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(11, len(smooth_mae_history) + 11), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this plot, it seems that validation MAE stops improving significantly after 80-90 epochs. Past that point, we start overfitting.\n",
    "\n",
    "Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we can train a final \"production\" model on all of the training data, with the best parameters, then look at its performance on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = build_model()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6140286922454834"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still off by about \\\\$2,600 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.839417],\n",
       "       [20.05707 ],\n",
       "       [22.035736],\n",
       "       [34.32895 ],\n",
       "       [25.849102],\n",
       "       [21.96819 ],\n",
       "       [28.503716],\n",
       "       [21.364992],\n",
       "       [21.170486],\n",
       "       [22.673906],\n",
       "       [17.766834],\n",
       "       [17.908905],\n",
       "       [16.72816 ],\n",
       "       [43.96984 ],\n",
       "       [20.687086],\n",
       "       [20.74221 ],\n",
       "       [26.215515],\n",
       "       [19.400566],\n",
       "       [19.922176],\n",
       "       [26.064777],\n",
       "       [12.476087],\n",
       "       [14.119743],\n",
       "       [22.101042],\n",
       "       [16.836277],\n",
       "       [20.49749 ],\n",
       "       [25.657612],\n",
       "       [30.236465],\n",
       "       [30.918898],\n",
       "       [11.75483 ],\n",
       "       [19.691519],\n",
       "       [20.413204],\n",
       "       [16.240543],\n",
       "       [33.964424],\n",
       "       [24.102936],\n",
       "       [17.972095],\n",
       "       [ 9.024531],\n",
       "       [17.135395],\n",
       "       [17.139038],\n",
       "       [20.391048],\n",
       "       [26.75076 ],\n",
       "       [31.699686],\n",
       "       [28.658102],\n",
       "       [13.991808],\n",
       "       [44.7191  ],\n",
       "       [28.91957 ],\n",
       "       [26.65857 ],\n",
       "       [27.154413],\n",
       "       [18.255306],\n",
       "       [23.585281],\n",
       "       [23.644371],\n",
       "       [35.76741 ],\n",
       "       [20.649782],\n",
       "       [11.819324],\n",
       "       [15.312027],\n",
       "       [36.982735],\n",
       "       [28.88712 ],\n",
       "       [12.215564],\n",
       "       [49.33091 ],\n",
       "       [35.29866 ],\n",
       "       [24.634754],\n",
       "       [25.720676],\n",
       "       [16.825272],\n",
       "       [16.317566],\n",
       "       [19.578493],\n",
       "       [23.868477],\n",
       "       [22.018196],\n",
       "       [13.739419],\n",
       "       [22.540627],\n",
       "       [13.797915],\n",
       "       [ 7.039466],\n",
       "       [26.221498],\n",
       "       [29.853024],\n",
       "       [26.8106  ],\n",
       "       [14.610623],\n",
       "       [25.295998],\n",
       "       [18.6272  ],\n",
       "       [19.207157],\n",
       "       [25.18684 ],\n",
       "       [36.27758 ],\n",
       "       [11.454793],\n",
       "       [22.32069 ],\n",
       "       [38.922867],\n",
       "       [17.289454],\n",
       "       [13.236518],\n",
       "       [18.644114],\n",
       "       [17.232067],\n",
       "       [21.741772],\n",
       "       [21.954924],\n",
       "       [22.622227],\n",
       "       [32.208   ],\n",
       "       [20.200151],\n",
       "       [20.324203],\n",
       "       [26.672794],\n",
       "       [43.98345 ],\n",
       "       [37.290092],\n",
       "       [20.975906],\n",
       "       [36.49913 ],\n",
       "       [51.473343],\n",
       "       [26.035095],\n",
       "       [48.67865 ],\n",
       "       [32.166252],\n",
       "       [21.319138]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* For regression problem, Mean Squared Error (`mse`) is a commonly used loss function and Mean Absolute Error (`mae`) is a common regression metric.\n",
    "* When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
    "* When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two), in order to avoid severe overfitting.\n",
    "* When there is little data available, using K-Fold cross validation is a great way to reliably evaluate a model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
